{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"colab":{"provenance":[]},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":9974726,"sourceType":"datasetVersion","datasetId":6137134}],"dockerImageVersionId":30787,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install einops","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-21T16:52:05.314676Z","iopub.execute_input":"2024-11-21T16:52:05.315033Z","iopub.status.idle":"2024-11-21T16:52:13.340901Z","shell.execute_reply.started":"2024-11-21T16:52:05.314999Z","shell.execute_reply":"2024-11-21T16:52:13.340000Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: einops in /opt/conda/lib/python3.10/site-packages (0.8.0)\n","output_type":"stream"}],"execution_count":36},{"cell_type":"code","source":"import tqdm\nimport random\nimport collections\nfrom pathlib import Path\nimport shutil\n\nimport cv2\nimport einops\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport math\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nimport torchvision.transforms as T\nfrom torch.utils.data import Dataset, DataLoader","metadata":{"id":"EYmpx85h8d7W","trusted":true,"execution":{"iopub.status.busy":"2024-11-21T17:25:44.714379Z","iopub.execute_input":"2024-11-21T17:25:44.715062Z","iopub.status.idle":"2024-11-21T17:25:44.719844Z","shell.execute_reply.started":"2024-11-21T17:25:44.715027Z","shell.execute_reply":"2024-11-21T17:25:44.718923Z"}},"outputs":[],"execution_count":61},{"cell_type":"code","source":"def list_files_per_class(root_dir):\n    \"\"\"\n    List the files in each class of the dataset given the root directory.\n\n    Args:\n      root_dir: Root directory where files are unzipped.\n\n    Return:\n      files: List of files in each of the classes.\n    \"\"\"\n    files = []\n    root_path = Path(root_dir)\n    for file_path in root_path.rglob('*'):  # Recursively list all files\n        if file_path.is_file():\n            files.append(str(file_path.relative_to(root_path)))\n    return files\n\ndef get_class(fname):\n  \"\"\"\n    Retrieve the name of the class given a filename.\n\n    Args:\n      fname: Name of the file in the UCF50 dataset.\n\n    Return:\n      Class that the file belongs to.\n  \"\"\"\n  return fname.split('_')[-3]\n\ndef get_files_per_class(files):\n  \"\"\"\n    Retrieve the files that belong to each class.\n\n    Args:\n      files: List of files in the dataset.\n\n    Return:\n      Dictionary of class names (key) and files (values).\n  \"\"\"\n  files_for_class = collections.defaultdict(list)\n  for fname in files:\n    class_name = get_class(fname)\n    files_for_class[class_name].append(fname)\n  return files_for_class\n\n\ndef move_files_to_class_dirs(root_dir, dest_dir, file_names):\n    \"\"\"\n    Organize files into class directories.\n\n    Args:\n      root_dir: Root directory where the files are currently stored.\n      dest_dir: Directory to organize files into class-based subdirectories.\n      file_names: Names of files to move.\n    \"\"\"\n    root_path = Path(root_dir)\n    dest_path = Path(dest_dir)\n\n    for fname in tqdm.tqdm(file_names):\n        class_name = get_class(fname)\n        src_file = root_path / fname\n        dest_class_dir = dest_path / class_name\n        dest_class_dir.mkdir(parents=True, exist_ok=True)\n        dest_file = dest_class_dir / src_file.name\n        shutil.copy2(str(src_file), str(dest_file))\n        \n\ndef split_class_lists(files_for_class, count):\n  \"\"\"\n    Returns the list of files belonging to a subset of data as well as the remainder of\n    files that need to be downloaded.\n\n    Args:\n      files_for_class: Files belonging to a particular class of data.\n      count: Number of files to download.\n\n    Return:\n      split_files: Files belonging to the subset of data.\n      remainder: Dictionary of the remainder of files that need to be downloaded.\n  \"\"\"\n  split_files = []\n  remainder = {}\n  for cls in files_for_class:\n    split_files.extend(files_for_class[cls][:count])\n    remainder[cls] = files_for_class[cls][count:]\n  return split_files, remainder\n\ndef organize_dataset(root_dir, num_classes, splits, dest_dir):\n    \"\"\"\n    Organize a dataset into class-based directories and split them into various parts.\n\n    Args:\n      root_dir: Root directory where files are currently unzipped.\n      num_classes: Number of labels.\n      splits: Dictionary specifying the training, validation, test, etc. (key) division of data\n              (value is number of files per split).\n      dest_dir: Directory to organize data into.\n\n    Return:\n      dir: Dictionary of paths to resulting directories for each split.\n    \"\"\"\n    files = list_files_per_class(root_dir)\n    files_for_class = get_files_per_class(files)\n    \n    classes = list(files_for_class.keys())[:num_classes]\n    \n    for cls in classes:\n        random.shuffle(files_for_class[cls])\n    \n    # Filter for the desired number of classes\n    files_for_class = {cls: files_for_class[cls] for cls in classes}\n    \n    dirs = {}\n    for split_name, split_count in splits.items():\n        print(split_name, \":\")\n        split_dir = Path(dest_dir) / split_name\n        split_files, files_for_class = split_class_lists(files_for_class, split_count)\n        move_files_to_class_dirs(root_dir, split_dir, split_files)\n        dirs[split_name] = split_dir\n    \n    return dirs\n\n\n\ndef format_frames(frame, output_size):\n    \"\"\"\n    Pad and resize an image from a video.\n\n    Args:\n      frame: Image that needs to be resized and padded.\n      output_size: Pixel size of the output frame image (height, width).\n\n    Returns:\n      Formatted frame with padding of specified output size.\n    \"\"\"\n    frame = frame.astype(float) / 255.0\n    frame = np.transpose(frame, (2, 0, 1))\n    transform = T.Compose([\n        T.Resize(output_size)\n    ])\n    frame = transform(torch.from_numpy(frame))\n    return frame\n\n\ndef set_frame_step(video_length, n_frames):\n    frame_step = math.floor(video_length / n_frames)\n    return frame_step\n\n\ndef frames_from_video_file(video_path, n_frames, output_size = (224,224), frame_step=15):\n  \"\"\"\n    Creates frames from each video file present for each category.\n\n    Args:\n      video_path: File path to the video.\n      n_frames: Number of frames to be created per video file.\n      output_size: Pixel size of the output frame image.\n\n    Return:\n      An NumPy array of frames in the shape of (n_frames, height, width, channels).\n  \"\"\"\n  # Read each video frame by frame\n  result = []\n  src = cv2.VideoCapture(str(video_path))\n\n  video_length = src.get(cv2.CAP_PROP_FRAME_COUNT)\n\n  if frame_step < 0:\n    frame_step = set_frame_step(video_length, n_frames)\n\n  need_length = 1 + (n_frames - 1) * frame_step\n\n  if need_length > video_length:\n    start = 0\n  else:\n    max_start = video_length - need_length\n    start = random.randint(0, max_start + 1)\n\n  src.set(cv2.CAP_PROP_POS_FRAMES, start)\n  # ret is a boolean indicating whether read was successful, frame is the image itself\n  ret, frame = src.read()\n  result.append(format_frames(frame, output_size))\n\n  for i in range(n_frames - 1):\n    for j in range(frame_step):\n      ret, frame = src.read()\n    if ret:\n      frame = format_frames(frame, output_size)\n      result.append(frame)\n    else:\n      result.append(np.zeros_like(result[0]))\n  src.release()\n  result = np.array(result)#[..., [2, 1, 0]]\n  return np.transpose(result, (1, 2, 3, 0))\n\n\nclass FrameGeneratorDataset(Dataset):\n    def __init__(self, path, n_frames, training=False, frame_step=15):\n        \"\"\"\n        Returns a set of frames with their associated label.\n\n        Args:\n          path: Directory path containing video files.\n          n_frames: Number of frames to extract from each video.\n          training: Boolean indicating if this is the training dataset (to enable shuffling).\n        \"\"\"\n        self.path = path\n        self.n_frames = n_frames\n        self.training = training\n        self.frame_step = frame_step\n\n        # Get all class names and assign a unique ID to each\n        self.class_names = sorted(set(p.name for p in self.path.iterdir() if p.is_dir()))\n        self.class_ids_for_name = {name: idx for idx, name in enumerate(self.class_names)}\n\n        # Get video file paths and their corresponding class labels\n        self.video_paths, self.classes = self.get_files_and_class_names()\n\n        # Shuffle the pairs if in training mode\n        if self.training:\n            data = list(zip(self.video_paths, self.classes))\n            random.shuffle(data)\n            self.video_paths, self.classes = zip(*data)\n\n    def get_files_and_class_names(self):\n        video_paths = list(self.path.glob('*/*.avi'))\n        classes = [p.parent.name for p in video_paths]\n        return video_paths, classes\n\n    def __len__(self):\n        # Return the number of video files available\n        return len(self.video_paths)\n\n    def __getitem__(self, idx):\n        \"\"\"\n        Retrieve frames and the label for a given video index.\n\n        Args:\n          idx: Index of the video to retrieve.\n\n        Returns:\n          A tuple (frames, label) where frames are the extracted frames from the video,\n          and label is the encoded class label.\n        \"\"\"\n        path = self.video_paths[idx]\n        class_name = self.classes[idx]\n\n        # Extract frames from the video file\n        video_frames = frames_from_video_file(path, self.n_frames, frame_step=self.frame_step)\n\n        # Get label by encoding the class name\n        label = self.class_ids_for_name[class_name]\n\n        # Convert frames and label to tensors\n        video_frames = torch.tensor(video_frames, dtype=torch.float32)\n        label = torch.tensor(label, dtype=torch.int64)\n\n        return video_frames, label","metadata":{"id":"3UH3LRbS8HSc","trusted":true,"execution":{"iopub.status.busy":"2024-11-21T17:25:51.686571Z","iopub.execute_input":"2024-11-21T17:25:51.686936Z","iopub.status.idle":"2024-11-21T17:25:51.709191Z","shell.execute_reply.started":"2024-11-21T17:25:51.686905Z","shell.execute_reply":"2024-11-21T17:25:51.708257Z"}},"outputs":[],"execution_count":62},{"cell_type":"code","source":"root_dir = '/kaggle/input/ucf50dataset/UCF50'\ndest_dir = '/kaggle/working/UCF50'\nnum_classes = 50\nsubset_paths = organize_dataset(root_dir,\n                        num_classes = num_classes,\n                        splits = {\"train\": 70, \"val\": 15, \"test\": 15},\n                        dest_dir = dest_dir)","metadata":{"id":"_UN2DthzA7-n","outputId":"83905118-093e-435d-a872-6d4dcfebe416","trusted":true,"execution":{"iopub.status.busy":"2024-11-21T17:26:01.229970Z","iopub.execute_input":"2024-11-21T17:26:01.230294Z","iopub.status.idle":"2024-11-21T17:26:13.843126Z","shell.execute_reply.started":"2024-11-21T17:26:01.230264Z","shell.execute_reply":"2024-11-21T17:26:13.842258Z"}},"outputs":[{"name":"stdout","text":"train :\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 70/70 [00:00<00:00, 393.41it/s]\n","output_type":"stream"},{"name":"stdout","text":"val :\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 15/15 [00:00<00:00, 552.81it/s]\n","output_type":"stream"},{"name":"stdout","text":"test :\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 15/15 [00:00<00:00, 482.83it/s]\n","output_type":"stream"}],"execution_count":63},{"cell_type":"code","source":"# Parameters\nn_frames = 10\nbatch_size = 8\n\n# Dataset paths\ntrain_dataset = FrameGeneratorDataset(subset_paths['train'], n_frames, training=True, frame_step=-1)\nval_dataset = FrameGeneratorDataset(subset_paths['val'], n_frames, training=False, frame_step=-1)\ntest_dataset = FrameGeneratorDataset(subset_paths['test'], n_frames, training=False, frame_step=-1)\n\n# DataLoaders with batching\ntrain_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\nval_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\ntest_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)","metadata":{"id":"_VHmav_ICfGP","trusted":true,"execution":{"iopub.status.busy":"2024-11-21T17:26:15.812607Z","iopub.execute_input":"2024-11-21T17:26:15.813356Z","iopub.status.idle":"2024-11-21T17:26:15.851240Z","shell.execute_reply.started":"2024-11-21T17:26:15.813313Z","shell.execute_reply":"2024-11-21T17:26:15.850438Z"}},"outputs":[],"execution_count":64},{"cell_type":"code","source":"# Define the dimensions of one frame in the set of frames created\nHEIGHT = 224\nWIDTH = 224","metadata":{"id":"YVeE3AVtC11c","trusted":true,"execution":{"iopub.status.busy":"2024-11-21T17:26:17.712105Z","iopub.execute_input":"2024-11-21T17:26:17.712437Z","iopub.status.idle":"2024-11-21T17:26:17.716590Z","shell.execute_reply.started":"2024-11-21T17:26:17.712408Z","shell.execute_reply":"2024-11-21T17:26:17.715693Z"}},"outputs":[],"execution_count":65},{"cell_type":"code","source":"class Conv2Plus1D(nn.Module):\n    def __init__(self, input_channels, filters, kernel_size, device):\n        \"\"\"\n        A sequence of convolutional layers that first apply the convolution operation over the\n        spatial dimensions, and then the temporal dimension.\n\n        Args:\n          filters (int): Number of filters in each convolutional layer.\n          kernel_size (tuple): Kernel size for the convolution in (depth, height, width).\n        \"\"\"\n        super(Conv2Plus1D, self).__init__()\n\n        padding_spatial = (0, kernel_size[1] // 2, kernel_size[2] // 2)\n        padding_temporal = (kernel_size[0] // 2, 0, 0)\n\n        # Sequential container for spatial and temporal convolution\n        self.seq = nn.Sequential(\n            # Spatial decomposition\n            nn.Conv3d(in_channels=input_channels,\n                      out_channels=filters,\n                      kernel_size=(1, kernel_size[1], kernel_size[2]),\n                      padding=padding_spatial,\n                      device=device),\n            # Temporal decomposition\n            nn.Conv3d(in_channels=filters,\n                      out_channels=filters,\n                      kernel_size=(kernel_size[0], 1, 1),\n                      padding=padding_temporal,\n                      device=device)\n        )\n\n    def forward(self, x):\n        return self.seq(x)","metadata":{"id":"BLao_9T5DG6u","trusted":true,"execution":{"iopub.status.busy":"2024-11-21T17:26:24.270196Z","iopub.execute_input":"2024-11-21T17:26:24.270929Z","iopub.status.idle":"2024-11-21T17:26:24.277102Z","shell.execute_reply.started":"2024-11-21T17:26:24.270896Z","shell.execute_reply":"2024-11-21T17:26:24.276212Z"}},"outputs":[],"execution_count":66},{"cell_type":"code","source":"class ResidualMain(nn.Module):\n    \"\"\"\n    Residual block of the model with convolution, layer normalization, and ReLU activation.\n    \"\"\"\n    def __init__(self, input_channels, filters, kernel_size, height, width, device):\n        super(ResidualMain, self).__init__()\n        self.conv1 = Conv2Plus1D(input_channels=input_channels, filters=filters, kernel_size=kernel_size, device=device)\n        self.norm = nn.LayerNorm(normalized_shape=(filters, ), eps=1e-3, device=device)\n        self.conv2 = Conv2Plus1D(input_channels=filters, filters=filters, kernel_size=kernel_size, device=device)\n        self.relu = nn.ReLU()\n\n    def forward(self, x):\n        # First convolution\n        x = self.conv1(x)\n        x = x.permute(0, 4, 2, 3, 1)\n        x = self.norm(x)\n        x = x.permute(0, 4, 2, 3, 1)\n\n        # Apply ReLU\n        x = self.relu(x)\n\n        # Second convolution\n        x = self.conv2(x)\n\n        x = x.permute(0, 4, 2, 3, 1)\n        x = self.norm(x)\n        x = x.permute(0, 4, 2, 3, 1)\n\n        return x","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-21T17:26:26.727635Z","iopub.execute_input":"2024-11-21T17:26:26.728764Z","iopub.status.idle":"2024-11-21T17:26:26.738677Z","shell.execute_reply.started":"2024-11-21T17:26:26.728728Z","shell.execute_reply":"2024-11-21T17:26:26.737628Z"}},"outputs":[],"execution_count":67},{"cell_type":"code","source":"class Project(nn.Module):\n    \"\"\"\n    Project certain dimensions of the tensor as the data is passed through different\n    sized filters and downsampled.\n    \"\"\"\n    def __init__(self, units, height, width, device):\n        super(Project, self).__init__()\n        self.seq = nn.Sequential(\n            nn.Linear(in_features=units[0], out_features=units[1], device=device),  # Linear projection\n            nn.LayerNorm(units[1], device=device)\n        )\n\n    def forward(self, x):\n        return self.seq(x)\n","metadata":{"id":"sBM1QoaoFX9R","trusted":true,"execution":{"iopub.status.busy":"2024-11-21T17:26:28.928345Z","iopub.execute_input":"2024-11-21T17:26:28.928706Z","iopub.status.idle":"2024-11-21T17:26:28.934228Z","shell.execute_reply.started":"2024-11-21T17:26:28.928676Z","shell.execute_reply":"2024-11-21T17:26:28.933219Z"}},"outputs":[],"execution_count":68},{"cell_type":"code","source":"def resize_video(video, height, width):\n    \"\"\"\n    Resize the tensor using einops and PyTorch's interpolation.\n\n    Args:\n      video (torch.Tensor): Tensor representation of the video, as a batch of frames.\n\n    Returns:\n      torch.Tensor: Resized video tensor with the new height and width.\n    \"\"\"\n    # Parse the shape and rearrange for resizing\n    old_shape = einops.parse_shape(video, 'b c h w t')\n    images = einops.rearrange(video, 'b c h w t -> b t h w c')\n    images = einops.rearrange(images, 'b t h w c -> (b t) h w c')\n\n    # Resize images using interpolate\n    images = F.interpolate(images.permute(0, 3, 1, 2), size=(height, width), mode='bilinear', align_corners=False)\n    images = images.permute(0, 2, 3, 1)  # Convert back to (batch, height, width, channels)\n\n    # Reshape back to video format\n    videos = einops.rearrange(images, '(b t) h w c -> b t h w c', t=old_shape['t'])\n    output = einops.rearrange(videos, 'b t h w c -> b c h w t')\n    return output\n","metadata":{"id":"NxQgYFVSF5CJ","trusted":true,"execution":{"iopub.status.busy":"2024-11-21T17:26:31.648301Z","iopub.execute_input":"2024-11-21T17:26:31.648640Z","iopub.status.idle":"2024-11-21T17:26:31.654450Z","shell.execute_reply.started":"2024-11-21T17:26:31.648612Z","shell.execute_reply":"2024-11-21T17:26:31.653537Z"}},"outputs":[],"execution_count":69},{"cell_type":"code","source":"class VideoModel(nn.Module):\n    def __init__(self, height, width, device):\n        super(VideoModel, self).__init__()\n        self.height = height\n        self.width = width\n        kernel_size = (3,3,3)\n\n        self.conv = Conv2Plus1D(input_channels=3, filters=16, kernel_size=(3, 7, 7), device=device)\n        self.norm = nn.LayerNorm(normalized_shape=(16, ), eps=1e-3, device=device)\n        self.relu = nn.ReLU()\n\n        self.residual1 = ResidualMain(16, 16, kernel_size, self.height // 2, self.width // 2, device)\n        self.residual2 = ResidualMain(16, 32, kernel_size, self.height // 4, self.width // 4, device)\n        self.residual3 = ResidualMain(32, 64, kernel_size, self.height // 8, self.width // 8, device)\n        self.residual4 = ResidualMain(64, 128, kernel_size, self.height // 16, self.width // 16, device)\n\n        self.project1 = Project(units=(16, 32), height=self.height // 2, width=self.width // 2, device=device)\n        self.project2 = Project(units=(32, 64), height=self.height // 4, width=self.width // 4, device=device)\n        self.project3 = Project(units=(64, 128), height=self.height // 8, width=self.width // 8, device=device)\n\n        # Pooling and Output Layers\n        self.global_avg_pool = nn.AdaptiveAvgPool3d((1, 1, 1))\n        self.flatten = nn.Flatten()\n        self.fc = nn.Linear(128, num_classes)  # Adjust the input to `Linear` based on the final channel size\n\n    def forward(self, x):\n        x = self.conv(x)\n        x = x.permute(0, 4, 2, 3, 1)\n        x = self.norm(x)\n        x = x.permute(0, 4, 2, 3, 1)\n        x = self.relu(x)\n\n        x = resize_video(x, self.height // 2, self.width // 2)\n        out = self.residual1(x)\n        x = torch.add(x, out)\n\n        resized_x = resize_video(x, self.height // 4, self.width // 4)\n        projected_x = self.project1(resized_x.permute(0, 4, 2, 3, 1))\n        residual_x = self.residual2(resized_x)\n        x = torch.add(projected_x.permute(0, 4, 2, 3, 1), residual_x)\n        \n        resized_x = resize_video(x, self.height // 8, self.width // 8)\n        projected_x = self.project2(resized_x.permute(0, 4, 2, 3, 1))\n        residual_x = self.residual3(resized_x)\n        x = torch.add(projected_x.permute(0, 4, 2, 3, 1), residual_x)\n        \n        resized_x = resize_video(x, self.height // 16, self.width // 16)\n        projected_x = self.project3(resized_x.permute(0, 4, 2, 3, 1))\n        residual_x = self.residual4(resized_x)\n        x = torch.add(projected_x.permute(0, 4, 2, 3, 1), residual_x)\n\n        # Global average pooling and flatten\n        x = self.global_avg_pool(x)\n        x = self.flatten(x)\n        x = self.fc(x)\n\n        return x","metadata":{"id":"CkBcFgq3Gfv_","trusted":true,"execution":{"iopub.status.busy":"2024-11-21T17:26:33.792217Z","iopub.execute_input":"2024-11-21T17:26:33.792713Z","iopub.status.idle":"2024-11-21T17:26:33.811141Z","shell.execute_reply.started":"2024-11-21T17:26:33.792669Z","shell.execute_reply":"2024-11-21T17:26:33.810250Z"}},"outputs":[],"execution_count":70},{"cell_type":"code","source":"def evaluate_model(model, val_loader, criterion):\n    model.eval()\n    val_loss = 0.0\n    correct = 0\n    total = 0\n    with torch.no_grad():\n        for frames, labels in val_loader:\n            frames, labels = frames.to(device), labels.to(device)\n            outputs = model(frames)\n            loss = criterion(outputs, labels)\n            val_loss += loss.item()\n\n            _, predicted = torch.max(outputs, 1)\n            total += labels.size(0)\n            correct += (predicted == labels).sum().item()\n    val_loss = val_loss / len(val_loader)\n    val_accuracy = correct / total\n    return val_loss, val_accuracy\n    \n\n# Training function\ndef train_model(model, optimizer, criterion, train_loader, val_loader, epochs=50):\n    history = {'train_loss': [], 'train_accuracy': [], 'val_loss': [], 'val_accuracy': []}\n    best = 0\n    for epoch in range(epochs):\n        model.train()\n        running_loss = 0.0\n        correct = 0\n        total = 0\n        for frames, labels in train_loader:\n            frames, labels = frames.to(device), labels.to(device)\n\n            optimizer.zero_grad()\n            outputs = model(frames)\n            loss = criterion(outputs, labels)\n            loss.backward()\n            optimizer.step()\n\n            running_loss += loss.item()\n\n            _, predicted = torch.max(outputs, 1)\n            correct += (predicted == labels).sum().item()\n            total += labels.size(0)\n\n            \n        train_accuracy = correct / total\n        # Validation phase\n        val_loss, val_accuracy = evaluate_model(model, val_loader, criterion)\n\n        train_loss = running_loss / len(train_loader)\n\n        if val_accuracy > best:\n              torch.save(model.state_dict(), \"best.pt\")\n              best = val_accuracy\n        history['train_loss'].append(train_loss)\n        history['train_accuracy'].append(train_accuracy)\n        history['val_loss'].append(val_loss)\n        history['val_accuracy'].append(val_accuracy)\n\n        print(f\"Epoch [{epoch+1}/{epochs}], Train Loss: {train_loss:.4f}, Train Acc: {train_accuracy:.4f}, Val Loss: {val_loss:.4f}, Val Acc: {val_accuracy:.4f}\")\n\n    return history","metadata":{"id":"uyzE8FqDHYQ8","trusted":true,"execution":{"iopub.status.busy":"2024-11-21T17:26:37.475335Z","iopub.execute_input":"2024-11-21T17:26:37.476296Z","iopub.status.idle":"2024-11-21T17:26:37.486412Z","shell.execute_reply.started":"2024-11-21T17:26:37.476257Z","shell.execute_reply":"2024-11-21T17:26:37.485420Z"}},"outputs":[],"execution_count":71},{"cell_type":"code","source":"import torch.optim as optim\n\n# Set up the device\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel = VideoModel(height=HEIGHT, width=WIDTH, device=device).to(device)\n\n# Define loss function and optimizer\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.Adam(model.parameters(), lr=0.0001)","metadata":{"id":"AtTwb_cFIuLa","trusted":true,"execution":{"iopub.status.busy":"2024-11-21T17:26:41.310623Z","iopub.execute_input":"2024-11-21T17:26:41.310954Z","iopub.status.idle":"2024-11-21T17:26:41.324402Z","shell.execute_reply.started":"2024-11-21T17:26:41.310926Z","shell.execute_reply":"2024-11-21T17:26:41.323634Z"}},"outputs":[],"execution_count":72},{"cell_type":"code","source":"# Training\n\nhistory = train_model(model, optimizer, criterion, train_loader, val_loader)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import numpy as np\nimport matplotlib.pyplot as plt\n\n\n# Plotting training losses and accuracy\nplt.figure(figsize=(12, 5))\n\nplt.subplot(1, 2, 1)\nplt.plot(history[\"train_loss\"], label='Train Loss')\nplt.plot(history[\"val_loss\"], label='Val Loss')\nplt.title('Training Loss')\nplt.xlabel('Epochs')\nplt.ylabel('Loss')\nplt.legend()\n\nplt.subplot(1, 2, 2)\nplt.plot(history[\"train_accuracy\"], label='Train Accuracy', color='green')\nplt.plot(history[\"val_accuracy\"], label='Val Accuracy', color='green')\nplt.title('Training Accuracy')\nplt.xlabel('Epochs')\nplt.ylabel('Accuracy')\nplt.legend()\n\nplt.tight_layout()\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-19T15:29:51.015329Z","iopub.execute_input":"2024-11-19T15:29:51.016359Z","iopub.status.idle":"2024-11-19T15:29:51.429890Z","shell.execute_reply.started":"2024-11-19T15:29:51.016312Z","shell.execute_reply":"2024-11-19T15:29:51.429089Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Generate actual and predicted labels\ndef get_actual_predicted_labels(model, dataset_loader):\n    model.eval()\n    actual = []\n    predicted = []\n    all_probas = []\n    with torch.no_grad():\n        for frames, labels in dataset_loader:\n            frames = frames.to(device)\n            outputs = model(frames)\n            _, preds = torch.max(outputs, 1)\n            actual.append(labels.cpu())\n            predicted.append(preds.cpu())\n            \n            # Apply softmax to get probabilities\n            probas = F.softmax(outputs, dim=1)\n            all_probas.append(probas.cpu())\n\n    # Concatenate all batches into single arrays\n    all_probas = torch.cat(all_probas, dim=0).numpy()\n    actual = torch.cat(actual, dim=0).numpy()\n    predicted = torch.cat(predicted, dim=0).numpy()\n    return actual, predicted, all_probas","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-19T15:24:07.825480Z","iopub.execute_input":"2024-11-19T15:24:07.825842Z","iopub.status.idle":"2024-11-19T15:24:07.832410Z","shell.execute_reply.started":"2024-11-19T15:24:07.825811Z","shell.execute_reply":"2024-11-19T15:24:07.831430Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"evaluate_model(model, test_loader, criterion)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-19T15:28:17.284411Z","iopub.execute_input":"2024-11-19T15:28:17.285185Z","iopub.status.idle":"2024-11-19T15:28:24.131931Z","shell.execute_reply.started":"2024-11-19T15:28:17.285152Z","shell.execute_reply":"2024-11-19T15:28:24.131016Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"y_true, y_pred, y_proba = get_actual_predicted_labels(model, test_loader)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-19T15:28:38.704553Z","iopub.execute_input":"2024-11-19T15:28:38.704903Z","iopub.status.idle":"2024-11-19T15:28:45.560240Z","shell.execute_reply.started":"2024-11-19T15:28:38.704872Z","shell.execute_reply":"2024-11-19T15:28:45.559459Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.metrics import precision_score, recall_score, roc_auc_score, roc_curve, auc, f1_score\n\n\n# Calculate precision and recall for each class\nprecision = precision_score(y_true, y_pred, average=None, zero_division=0)\nrecall = recall_score(y_true, y_pred, average=None, zero_division=0)\nf1 = f1_score(y_true, y_pred, average=None, zero_division=0)\n\nroc_auc_per_class = []\n\n# Plotting ROC curves for each class\nplt.figure(figsize=(12, 8))\nfor i in range(num_classes):\n    y_true_binary = (y_true == i).astype(int)\n    roc_auc = roc_auc_score(y_true_binary, y_proba[:, i])\n    roc_auc_per_class.append(roc_auc)\n    fpr, tpr, _ = roc_curve(y_true_binary, y_proba[:, i])\n    plt.plot(fpr, tpr, label=f'Class {i} (AUC = {roc_auc_per_class[i]:.2f})')\n\n# Macro average ROC-AUC\nroc_auc_macro = np.mean(roc_auc_per_class)\n\nplt.title('ROC-AUC Curve')\nplt.xlabel('False Positive Rate (FPR)')\nplt.ylabel('True Positive Rate (TPR)')\nplt.legend()\nplt.show()\n\n# Output metrics\nprint(\"F1 per class:\")\nfor i in range(num_classes):\n    print(\"Class\", i, \"-\", f1[i])\nprint()\n\nprint(\"Precision per class:\")\nfor i in range(num_classes):\n    print(\"Class\", i, \"-\", precision[i])\nprint()\n\nprint(\"Recall per class:\")\nfor i in range(num_classes):\n    print(\"Class\", i, \"-\", recall[i])\nprint()\n\nprint(\"ROC-AUC per class:\")\nfor i in range(num_classes):\n    print(\"Class\", i, \"-\", roc_auc_per_class[i])\n    \nprint(f\"Macro-average ROC-AUC: {roc_auc_macro:.2f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-19T15:28:52.899621Z","iopub.execute_input":"2024-11-19T15:28:52.899995Z","iopub.status.idle":"2024-11-19T15:28:53.722104Z","shell.execute_reply.started":"2024-11-19T15:28:52.899963Z","shell.execute_reply":"2024-11-19T15:28:53.721290Z"}},"outputs":[],"execution_count":null}]}